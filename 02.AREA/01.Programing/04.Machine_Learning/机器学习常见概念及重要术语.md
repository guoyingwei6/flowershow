---
created: 2024-06-04 20:58
updated: 2025-06-10 09:55
---


![image.png](https://picbed.guoyingwei.top/2024/08/202408102134544.png)

![image.png](https://picbed.guoyingwei.top/2024/08/202408102134723.png)

![image.png](https://picbed.guoyingwei.top/2024/08/202408102134742.png)




## 准确率
既然是个分类指标，我们可以很自然的想到**准确率，**准确率的定义是**预测正确的结果占总样本的百分比**，其公式如下：
Note
准确率=**(TP+TN)/(TP+TN+FP+FN)**

虽然准确率可以判断总的正确率，但是在**样本不平衡** 的情况下，并不能作为很好的指标来衡量结果。举个简单的例子，比如在一个总样本中，正样本占90%，负样本占10%，样本是严重不平衡的。对于这种情况，我们只需要将全部样本预测为正样本即可得到90%的高准确率，但实际上我们并没有很用心的分类，只是随便无脑一分而已。这就说明了：**由于样本不平衡的问题，导致了得到的高准确率结果含有很大的水分。即如果样本不平衡，准确率就会失效。**


## 精准率

精准率（Precision）又叫**查准率**，它是**针对预测结果** 而言的，它的含义是**在所有被预测为正的样本中实际为正的样本的概率**，意思就是在预测为正样本的结果中，我们有多少把握可以预测正确，其公式如下：

精准率=**TP/(TP+FP)**

精准率和准确率看上去有些类似，但是完全不同的两个概念。精准率代表对正样本结果中的预测准确程度，而准确率则代表整体的预测准确程度，既包括正样本，也包括负样本。

## 召回率

召回率（Recall）又叫**查全率**，它是**针对原样本**而言的，它的含义是**在实际为正的样本中被预测为正样本的概率**，其公式如下：

精准率=**TP/(TP+FN)**

**召回率的应用场景：** 比如拿网贷违约率为例，相对好用户，我们更关心坏用户，不能错放过任何一个坏用户。因为如果我们过多的将坏用户当成好用户，这样后续可能发生的违约金额会远超过好用户偿还的借贷利息金额，造成严重偿失。**召回率越高，代表实际坏用户被预测出来的概率越高，它的含义类似：宁可错杀一千，绝不放过一个。**


## F1分数

但通常，如果想要找到二者之间的一个**平衡点**，我们就需要一个新的指标：**F1分数**。F1分数同时考虑了查准率和查全率，让二者同时达到最高，取一个平衡。F1分数的公式为 = **2_查准率_查全率 / (查准率 + 查全率)。** 我们在图中看到的平衡点就是F1分数得来的结果。


## **灵敏度，特异度，真正率，假正率**

在正式介绍ROC/AUC之前，我们还要再介绍两个指标，**这两个指标的选择也正是ROC和AUC可以无视样本不平衡的原因。** 这两个指标分别是：**灵敏度和（1-特异度），也叫做真正率（TPR）和假正率（FPR）**。

灵敏度（Sensitivity） = **TP/(TP+FN)**

特异度（Specificity） = **TN/(FP+TN)**

- 其实我们可以发现灵敏度和召回率是一模一样的，只是名字换了而已。
- 由于我们比较关心正样本，所以需要查看有多少负样本被错误地预测为正样本，所以使用（1-特异度），而不是特异度。

真正率（TPR） = 灵敏度 = **TP/(TP+FN)**

假正率（FPR） = 1- 特异度 = **FP/(FP+TN)**

下面是真正率和假正率的示意，我们发现**TPR和FPR分别是基于实际表现1和0出发的，也就是说它们分别在实际的正样本和负样本中来观察相关概率问题。** 正因为如此，所以无论样本是否平衡，都不会被影响。还是拿之前的例子，总样本中，90%是正样本，10%是负样本。我们知道用准确率是有水分的，但是用TPR和FPR不一样。这里，TPR只关注90%正样本中有多少是被真正覆盖的，而与那10%毫无关系，同理，FPR只关注10%负样本中有多少是被错误覆盖的，也与那90%毫无关系，所以可以看出：**如果我们从实际表现的各个结果角度出发，就可以避免样本不平衡的问题了，这也是为什么选用TPR和FPR作为ROC/AUC的指标的原因。**

或者我们也可以从另一个角度考虑：**条件概率。** 我们假设**X**为预测值，**Y**为真实值。那么就可以将这些指标按条件概率表示：

**精准率 = P（Y=1 | X=1）**

**召回率 = 灵敏度 = P（X=1 | Y=1）**

**特异度 = P（X=0 | Y=0）**

从上面三个公式看到：**如果我们先以实际结果为条件（召回率，特异度），那么就只需考虑一种样本，而先以预测值为条件（精准率），那么我们需要同时考虑正样本和负样本。所以先以实际结果为条件的指标都不受样本不平衡的影响，相反以预测结果为条件的就会受到影响。**

## **ROC（接受者操作特征曲线）**

> ROC（Receiver Operating Characteristic）曲线，又称接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力，ROC曲线是基于混淆矩阵得出的。

ROC曲线中的主要两个指标就是**真正率**和**假正率，** 上面也解释了这么选择的好处所在。其中横坐标为假正率（FPR），纵坐标为真正率（TPR），下面就是一个标准的ROC曲线图。

![](https://img.6aiq.com/e/da2338e6537043aebce95d5e0e395164.jpeg)

### **ROC曲线的阈值问题**

与前面的P-R曲线类似，ROC曲线也是通过**遍历所有阈值** 来绘制整条曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在ROC曲线图中也会沿着曲线滑动。

![44a4458a88104df3af1f5e38d822fcd3-v2296b158ebb205a2b90d05f5d2074bbe9b.gif](https://img.6aiq.com/file/2019/2/44a4458a88104df3af1f5e38d822fcd3-v2296b158ebb205a2b90d05f5d2074bbe9b.gif)

### **如何判断ROC曲线的好坏？**

改变阈值只是不断地改变预测的正负样本数，即TPR和FPR，但是曲线本身是不会变的。那么如何判断一个模型的ROC曲线是好的呢？这个还是要回归到我们的目的：FPR表示模型虚报的响应程度，而TPR表示模型预测响应的覆盖程度。我们所希望的当然是：虚报的越少越好，覆盖的越多越好。所以总结一下就是**TPR越高，同时FPR越低（即ROC曲线越陡），那么模型的性能就越好。** 参考如下

### **ROC曲线无视样本不平衡**

前面已经对ROC曲线为什么可以无视样本不平衡做了解释，下面我们用动态图的形式再次展示一下它是如何工作的。我们发现：**无论红蓝色样本比例如何改变，ROC曲线都没有影响。**

## AUC（曲线下的面积）

为了计算 ROC 曲线上的点，我们可以使用不同的分类阈值多次评估逻辑回归模型，但这样做效率非常低。幸运的是，有一种基于排序的高效算法可以为我们提供此类信息，这种算法称为**曲线下面积（Area Under Curve）**。

比较有意思的是，如果我们连接对角线，它的面积正好是0.5。对角线的实际含义是：**随机判断响应与不响应，正负样本覆盖率应该都是50%，表示随机效果。** ROC曲线越陡越好，所以理想值就是1，一个正方形，而最差的随机判断都有0.5，所以一般AUC的值是介于0.5到1之间的。

### **AUC的一般判断标准**

**0.5 - 0.7：** 效果较低，但用于预测股票已经很不错了

**0.7 - 0.85：** 效果一般

**0.85 - 0.95：** 效果很好

**0.95 - 1：** 效果非常好，但一般不太可能

### **AUC的物理意义**

> 曲线下面积对所有可能的分类阈值的效果进行综合衡量。曲线下面积的一种解读方式是看作模型将某个随机正类别样本排列在某个随机负类别样本之上的概率。以下面的样本为例，逻辑回归预测从左到右以升序排列：

![](https://img.6aiq.com/e/90b76e111ab341af885ffa2be24b9ff5.jpeg)
